{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shernee/04_cmpe258/blob/master/Data_augmentation_classification_%5Bk%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install nlpaug\n",
        "!pip install opencv-python-headless\n",
        "!pip install av\n",
        "!pip install -U augly[video]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOVIVbHtm0jT",
        "outputId": "838cb797-397e-4307-b5c2-8079d6e8d955"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from opencv-python-headless) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-10.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting augly[video]\n",
            "  Downloading augly-1.0.0-py3-none-any.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iopath>=0.1.8\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex>=2021.4.4 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (2022.10.31)\n",
            "Collecting python-magic>=0.4.22\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torchaudio>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (2.0.1+cu118)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (3.0.0)\n",
            "Requirement already satisfied: Pillow<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (8.4.0)\n",
            "Requirement already satisfied: SoundFile>=0.10.3.post1 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (0.12.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (2.0.0+cu118)\n",
            "Collecting vidgear>=0.2.4\n",
            "  Downloading vidgear-0.3.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.5.2.54 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (4.7.0.72)\n",
            "Collecting ffmpeg-python>=0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: librosa>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from augly[video]) (0.10.0.post2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python>=0.2.0->augly[video]) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from iopath>=0.1.8->augly[video]) (4.65.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from iopath>=0.1.8->augly[video]) (4.5.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (0.56.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (1.10.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (0.3.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (4.4.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (1.0.5)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa>=0.8.1->augly[video]) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from SoundFile>=0.10.3.post1->augly[video]) (1.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->augly[video]) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->augly[video]) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->augly[video]) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->augly[video]) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.0->augly[video]) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.9.0->augly[video]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.9.0->augly[video]) (16.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vidgear>=0.2.4->augly[video]) (2.27.1)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from vidgear>=0.2.4->augly[video]) (0.29.34)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->SoundFile>=0.10.3.post1->augly[video]) (2.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa>=0.8.1->augly[video]) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa>=0.8.1->augly[video]) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa>=0.8.1->augly[video]) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa>=0.8.1->augly[video]) (23.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vidgear>=0.2.4->augly[video]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vidgear>=0.2.4->augly[video]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vidgear>=0.2.4->augly[video]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vidgear>=0.2.4->augly[video]) (2.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->librosa>=0.8.1->augly[video]) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.9.0->augly[video]) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.9.0->augly[video]) (1.3.0)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=d5b4f2070d0b2c4cfa395973afc1ba2b976db9672146443d79691a7679c47521\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/13/6d/441d8f2af76ee6d2a3e67eebb1d0c556fefcee0a8b32266a8e\n",
            "Successfully built iopath\n",
            "Installing collected packages: python-magic, portalocker, ffmpeg-python, colorlog, vidgear, iopath, augly\n",
            "Successfully installed augly-1.0.0 colorlog-6.7.0 ffmpeg-python-0.2.0 iopath-0.1.10 portalocker-2.7.0 python-magic-0.4.27 vidgear-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tsaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2m1nI1jDN6x",
        "outputId": "0f2c04aa-b7e0-4e07-85e1-b5ec93ab7ae9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tsaug\n",
            "  Downloading tsaug-0.2.1-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.9/dist-packages (from tsaug) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.9/dist-packages (from tsaug) (1.10.1)\n",
            "Installing collected packages: tsaug\n",
            "Successfully installed tsaug-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-magic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhK_Zg03qJQJ",
        "outputId": "45641d7a-fc09-4a38-cf08-6fa42ee84521"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  python3-magic\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 9,376 B of archives.\n",
            "After this operation, 43.0 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-magic all 2:0.4.15-3 [9,376 B]\n",
            "Fetched 9,376 B in 1s (14.1 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-magic.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-magic_2%3a0.4.15-3_all.deb ...\n",
            "Unpacking python3-magic (2:0.4.15-3) ...\n",
            "Setting up python3-magic (2:0.4.15-3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, GlobalMaxPooling1D, LSTM\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "import augly.video as vidaugs\n",
        "import tsaug\n",
        "from tsaug import TimeWarp, Crop, Drift, AddNoise, Reverse\n"
      ],
      "metadata": {
        "id": "Igo77M7imy8f"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular data"
      ],
      "metadata": {
        "id": "K1o01Sah5pAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8-VSzYmTtk-",
        "outputId": "dd3935cd-534c-4751-d48f-24fd48252035"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-12 18:17:07--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
            "Saving to: ‘default of credit card clients.xls’\n",
            "\n",
            "default of credit c 100%[===================>]   5.28M  4.48MB/s    in 1.2s    \n",
            "\n",
            "2023-04-12 18:17:09 (4.48 MB/s) - ‘default of credit card clients.xls’ saved [5539328/5539328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the data by splitting it into a training and testing set, normalizing the features, and one-hot encoding the target variable"
      ],
      "metadata": {
        "id": "o7aNt7SxUA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('default of credit card clients.xls', header=1)"
      ],
      "metadata": {
        "id": "6KRjgDNNm4Z2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the features and target variable\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder()\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
        "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "iMHNz92GT9Jl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Augmentation"
      ],
      "metadata": {
        "id": "bLGq2W7nUWXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUeBrNHmUEr4",
        "outputId": "281b699c-a182-493a-a22c-472c11ede47f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 4s 4ms/step - loss: 0.4702 - accuracy: 0.8087 - val_loss: 0.4492 - val_accuracy: 0.8183\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4404 - accuracy: 0.8183 - val_loss: 0.4383 - val_accuracy: 0.8192\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4338 - accuracy: 0.8196 - val_loss: 0.4383 - val_accuracy: 0.8190\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4297 - accuracy: 0.8208 - val_loss: 0.4396 - val_accuracy: 0.8177\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4275 - accuracy: 0.8225 - val_loss: 0.4373 - val_accuracy: 0.8188\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8228 - val_loss: 0.4404 - val_accuracy: 0.8142\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4236 - accuracy: 0.8217 - val_loss: 0.4354 - val_accuracy: 0.8168\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8227 - val_loss: 0.4363 - val_accuracy: 0.8192\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.4203 - accuracy: 0.8242 - val_loss: 0.4396 - val_accuracy: 0.8185\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.4200 - accuracy: 0.8233 - val_loss: 0.4403 - val_accuracy: 0.8178\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd52569070>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQydmr_6UQ2A",
        "outputId": "85a6f0e6-0307-498b-a3f4-32e593a559d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 1s 3ms/step - loss: 0.4403 - accuracy: 0.8178\n",
            "Test accuracy: 0.8178333044052124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to add noise to the data is by adding random Gaussian noise to each feature with a given mean and sd"
      ],
      "metadata": {
        "id": "y7SBel7BUZkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(feature, mean, std):\n",
        "    noise = np.random.normal(mean, std, len(feature))\n",
        "    return feature + noise\n",
        "\n",
        "# Add noise to each feature of the dataset\n",
        "for i in range(1, len(df.columns)):\n",
        "    df.iloc[:, i] = add_noise(df.iloc[:, i], 0, 0.1)"
      ],
      "metadata": {
        "id": "JSmSeK9IUIbt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "J6EoGeaGVYnR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=2)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "UVpOjzvwWMW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9f41b3-9f9c-4bd1-b031-de31b6165490"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "750/750 - 2s - loss: 0.4874 - accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
            "Epoch 2/50\n",
            "750/750 - 1s - loss: 0.4542 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 3/50\n",
            "750/750 - 1s - loss: 0.4452 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 4/50\n",
            "750/750 - 1s - loss: 0.4408 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 5/50\n",
            "750/750 - 1s - loss: 0.4393 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 6/50\n",
            "750/750 - 2s - loss: 0.4358 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 7/50\n",
            "750/750 - 1s - loss: 0.4344 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 8/50\n",
            "750/750 - 1s - loss: 0.4324 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 9/50\n",
            "750/750 - 1s - loss: 0.4311 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 10/50\n",
            "750/750 - 1s - loss: 0.4317 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "750/750 - 1s - loss: 0.4297 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "750/750 - 1s - loss: 0.4293 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "750/750 - 1s - loss: 0.4278 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 14/50\n",
            "750/750 - 1s - loss: 0.4271 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "750/750 - 1s - loss: 0.4256 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 16/50\n",
            "750/750 - 2s - loss: 0.4271 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "750/750 - 2s - loss: 0.4246 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 18/50\n",
            "750/750 - 1s - loss: 0.4238 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 19/50\n",
            "750/750 - 1s - loss: 0.4226 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 20/50\n",
            "750/750 - 1s - loss: 0.4246 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 21/50\n",
            "750/750 - 1s - loss: 0.4230 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 22/50\n",
            "750/750 - 1s - loss: 0.4227 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "750/750 - 1s - loss: 0.4222 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "750/750 - 1s - loss: 0.4215 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 25/50\n",
            "750/750 - 1s - loss: 0.4209 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "750/750 - 1s - loss: 0.4217 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "750/750 - 2s - loss: 0.4193 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "750/750 - 1s - loss: 0.4187 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "750/750 - 2s - loss: 0.4192 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "750/750 - 1s - loss: 0.4181 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "750/750 - 1s - loss: 0.4191 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 32/50\n",
            "750/750 - 1s - loss: 0.4175 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 33/50\n",
            "750/750 - 1s - loss: 0.4170 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 34/50\n",
            "750/750 - 1s - loss: 0.4157 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "750/750 - 1s - loss: 0.4164 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 36/50\n",
            "750/750 - 1s - loss: 0.4166 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "750/750 - 2s - loss: 0.4162 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "750/750 - 1s - loss: 0.4152 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "750/750 - 1s - loss: 0.4115 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 40/50\n",
            "750/750 - 1s - loss: 0.4130 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 41/50\n",
            "750/750 - 1s - loss: 0.4146 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 42/50\n",
            "750/750 - 1s - loss: 0.4127 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "750/750 - 1s - loss: 0.4131 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 44/50\n",
            "750/750 - 1s - loss: 0.4125 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 45/50\n",
            "750/750 - 1s - loss: 0.4109 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "750/750 - 1s - loss: 0.4123 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Epoch 47/50\n",
            "750/750 - 1s - loss: 0.4104 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "750/750 - 2s - loss: 0.4109 - accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
            "Epoch 49/50\n",
            "750/750 - 1s - loss: 0.4124 - accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
            "Epoch 50/50\n",
            "750/750 - 1s - loss: 0.4106 - accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
            "Test accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added random Gaussian noise with mean 0 and standard deviation 0.1 to each feature of the dataset, except for the target variable.\n",
        "\n",
        "Adding noise to the data can help in making the model more robust to noise in the input data, but it may not always improve the performance of the model.\n",
        "\n",
        "It reduces loss but worsens test accuracy"
      ],
      "metadata": {
        "id": "D411RoIBXjSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image"
      ],
      "metadata": {
        "id": "pmWggg-qnWv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "filename = 'cats_and_dogs_filtered.zip'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract dataset\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ],
      "metadata": {
        "id": "0n_I45DtnWOv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up directories\n",
        "base_dir = 'cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Set up data generators with data augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emnWGg7WoGsS",
        "outputId": "a9e3994f-e601-436c-f008-5a84c3149761"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37zD1JpvoKcZ",
        "outputId": "d535ac5f-a939-47cd-d7ec-91609c07c9a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=5,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=50)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B08Uk3VoMn8",
        "outputId": "139e983d-52e4-493a-e77b-84d93ffc7976"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-49310b23b6bb>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "50/50 [==============================] - 81s 2s/step - loss: 0.6973 - accuracy: 0.5080 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 81s 2s/step - loss: 0.6938 - accuracy: 0.5060 - val_loss: 0.7697 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 84s 2s/step - loss: 0.6927 - accuracy: 0.5220 - val_loss: 0.6873 - val_accuracy: 0.6110\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 81s 2s/step - loss: 0.6949 - accuracy: 0.5190 - val_loss: 0.6877 - val_accuracy: 0.5500\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 78s 2s/step - loss: 0.6992 - accuracy: 0.5240 - val_loss: 0.6722 - val_accuracy: 0.6350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-49310b23b6bb>:10: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.6349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text"
      ],
      "metadata": {
        "id": "ixYr-fx5oQW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1TOkYUoPuG",
        "outputId": "a591415c-4464-4a5a-b6dc-2d0e16a17a6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "metadata": {
        "id": "JbP2ipmZolx6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model using an embedding layer, a global max pooling layer, and two dense layers with dropout regularization\n",
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(10000, embedding_dim, input_length=maxlen),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vHDaBRsporhP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    partial_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=40,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n",
        "# During training, Keras will automatically apply data augmentation techniques like shuffling and random cropping to the input data to prevent overfitting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq207dkdpDw3",
        "outputId": "92f76625-5061-49f2-8524-3eb8b4c8867d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 2s 30ms/step - loss: 0.6927 - accuracy: 0.5178 - val_loss: 0.6914 - val_accuracy: 0.5571\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.6892 - accuracy: 0.5597 - val_loss: 0.6862 - val_accuracy: 0.6409\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.6804 - accuracy: 0.6211 - val_loss: 0.6745 - val_accuracy: 0.6476\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.6627 - accuracy: 0.6751 - val_loss: 0.6516 - val_accuracy: 0.7605\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.6322 - accuracy: 0.7307 - val_loss: 0.6139 - val_accuracy: 0.7990\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.5837 - accuracy: 0.7725 - val_loss: 0.5614 - val_accuracy: 0.8106\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.5262 - accuracy: 0.7969 - val_loss: 0.5014 - val_accuracy: 0.8183\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.4678 - accuracy: 0.8177 - val_loss: 0.4522 - val_accuracy: 0.8250\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.4183 - accuracy: 0.8383 - val_loss: 0.4172 - val_accuracy: 0.8287\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.3774 - accuracy: 0.8549 - val_loss: 0.3936 - val_accuracy: 0.8315\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.3519 - accuracy: 0.8658 - val_loss: 0.3790 - val_accuracy: 0.8340\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.3176 - accuracy: 0.8791 - val_loss: 0.3691 - val_accuracy: 0.8368\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.2963 - accuracy: 0.8919 - val_loss: 0.3624 - val_accuracy: 0.8404\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.2778 - accuracy: 0.8965 - val_loss: 0.3589 - val_accuracy: 0.8409\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.2558 - accuracy: 0.9112 - val_loss: 0.3562 - val_accuracy: 0.8423\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.2379 - accuracy: 0.9166 - val_loss: 0.3550 - val_accuracy: 0.8411\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.2212 - accuracy: 0.9270 - val_loss: 0.3555 - val_accuracy: 0.8421\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.2038 - accuracy: 0.9319 - val_loss: 0.3574 - val_accuracy: 0.8438\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1908 - accuracy: 0.9378 - val_loss: 0.3604 - val_accuracy: 0.8425\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.1786 - accuracy: 0.9429 - val_loss: 0.3641 - val_accuracy: 0.8437\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.1651 - accuracy: 0.9487 - val_loss: 0.3692 - val_accuracy: 0.8442\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.1556 - accuracy: 0.9517 - val_loss: 0.3733 - val_accuracy: 0.8448\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.1421 - accuracy: 0.9577 - val_loss: 0.3799 - val_accuracy: 0.8429\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.1320 - accuracy: 0.9618 - val_loss: 0.3872 - val_accuracy: 0.8432\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.1248 - accuracy: 0.9625 - val_loss: 0.3936 - val_accuracy: 0.8408\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.1172 - accuracy: 0.9658 - val_loss: 0.4004 - val_accuracy: 0.8410\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.1067 - accuracy: 0.9705 - val_loss: 0.4084 - val_accuracy: 0.8392\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.1008 - accuracy: 0.9719 - val_loss: 0.4166 - val_accuracy: 0.8395\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0952 - accuracy: 0.9755 - val_loss: 0.4238 - val_accuracy: 0.8383\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0869 - accuracy: 0.9788 - val_loss: 0.4338 - val_accuracy: 0.8369\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0828 - accuracy: 0.9805 - val_loss: 0.4436 - val_accuracy: 0.8364\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0758 - accuracy: 0.9829 - val_loss: 0.4524 - val_accuracy: 0.8368\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0742 - accuracy: 0.9831 - val_loss: 0.4605 - val_accuracy: 0.8341\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0664 - accuracy: 0.9859 - val_loss: 0.4728 - val_accuracy: 0.8353\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 34ms/step - loss: 0.0622 - accuracy: 0.9865 - val_loss: 0.4828 - val_accuracy: 0.8333\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.0577 - accuracy: 0.9873 - val_loss: 0.4940 - val_accuracy: 0.8332\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.0554 - accuracy: 0.9890 - val_loss: 0.5027 - val_accuracy: 0.8326\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0518 - accuracy: 0.9902 - val_loss: 0.5130 - val_accuracy: 0.8313\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0489 - accuracy: 0.9903 - val_loss: 0.5247 - val_accuracy: 0.8296\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0464 - accuracy: 0.9910 - val_loss: 0.5341 - val_accuracy: 0.8303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94wjzFgtpLve",
        "outputId": "eb787a31-3dc1-4919-f50f-ef44ac18c43c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6128 - accuracy: 0.8078\n",
            "[0.6128406524658203, 0.8077600002288818]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the augmentation function\n",
        "def augment_text(text, aug):\n",
        "    augmented_text = aug.augment(text)\n",
        "    return augmented_text\n",
        "\n",
        "# Define the augmentation method\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "\n",
        "# Augment the text data\n",
        "augmented_x_train = [aug.augment(str(text)) for text in partial_x_train]\n",
        "augmented_x_val = [aug.augment(str(text)) for text in x_val]\n",
        "augmented_test_data = [aug.augment(str(text)) for text in test_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkIkkc1mpRKN",
        "outputId": "07764877-1916-4ea4-e0ed-53a3493fbdd3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer object\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "augmented_x_train = tokenizer.texts_to_sequences(augmented_x_train)\n",
        "augmented_x_val = tokenizer.texts_to_sequences(augmented_x_val)\n",
        "augmented_test_data = tokenizer.texts_to_sequences(augmented_test_data)\n",
        "\n",
        "# Pad the augmented data\n",
        "augmented_x_train = pad_sequences(augmented_x_train, maxlen=maxlen)\n",
        "augmented_x_val = pad_sequences(augmented_x_val, maxlen=maxlen)\n",
        "augmented_test_data = pad_sequences(augmented_test_data, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "jXQsx_7CpdKC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with augmented data\n",
        "history = model.fit(\n",
        "    augmented_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=40,\n",
        "    batch_size=512,\n",
        "    validation_data=(augmented_x_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model with the augmented test data\n",
        "results = model.evaluate(augmented_test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lXL8Kyzpfl9",
        "outputId": "9c8db66b-2c3f-441b-e7e6-8f2a0a34a720"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.7361 - accuracy: 0.4971 - val_loss: 0.6964 - val_accuracy: 0.4947\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.6954 - accuracy: 0.5068 - val_loss: 0.6931 - val_accuracy: 0.5053\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6938 - accuracy: 0.5008 - val_loss: 0.6940 - val_accuracy: 0.4947\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6935 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 30ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 31ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4947\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 33ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4947\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.4947\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "[0.6931738257408142, 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video"
      ],
      "metadata": {
        "id": "P2_A7k4bpnTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
        "!unrar x hmdb51_org.rar\n",
        "!unrar x shoot_gun.rar\n",
        "!unrar x brush_hair.rar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPpGQ9oQpmPu",
        "outputId": "dbf00e25-0eb6-490f-c283-fdd9d8f7da2d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-12 19:23:01--  http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
            "Resolving serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)... 128.148.254.114\n",
            "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar [following]\n",
            "--2023-04-12 19:23:01--  https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
            "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "\n",
            "UNRAR 5.61 beta 1 freeware      Copyright (c) 1993-2018 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from hmdb51_org.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file shoot_gun.rar\n",
            "32372358 bytes, modified on 2011-08-25 12:21\n",
            "with a new one\n",
            "32372358 bytes, modified on 2011-08-25 12:21\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_gun.rar                                                \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file sit.rar\n",
            "41022787 bytes, modified on 2011-08-25 12:22\n",
            "with a new one\n",
            "41022787 bytes, modified on 2011-08-25 12:22\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  sit.rar                                                      \b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file situp.rar\n",
            "27753586 bytes, modified on 2011-08-25 12:22\n",
            "with a new one\n",
            "27753586 bytes, modified on 2011-08-25 12:22\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  situp.rar                                                    \b\b\b\b  3%\b\b\b\b  4%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file smile.rar\n",
            "17963800 bytes, modified on 2011-08-25 12:22\n",
            "with a new one\n",
            "17963800 bytes, modified on 2011-08-25 12:22\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y \n",
            "\n",
            "Extracting  smile.rar                                                    \b\b\b\b  4%\b\b\b\b  5%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file smoke.rar\n",
            "43034968 bytes, modified on 2011-08-25 12:23\n",
            "with a new one\n",
            "43034968 bytes, modified on 2011-08-25 12:23\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  smoke.rar                                                    \b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file somersault.rar\n",
            "35618370 bytes, modified on 2011-08-25 12:23\n",
            "with a new one\n",
            "35618370 bytes, modified on 2011-08-25 12:23\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  somersault.rar                                               \b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file stand.rar\n",
            "38862951 bytes, modified on 2011-08-25 12:23\n",
            "with a new one\n",
            "38862951 bytes, modified on 2011-08-25 12:23\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  stand.rar                                                    \b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file swing_baseball.rar\n",
            "58308188 bytes, modified on 2011-08-25 12:24\n",
            "with a new one\n",
            "58308188 bytes, modified on 2011-08-25 12:24\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  swing_baseball.rar                                           \b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file sword.rar\n",
            "35555921 bytes, modified on 2011-08-25 12:24\n",
            "with a new one\n",
            "35555921 bytes, modified on 2011-08-25 12:24\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  sword.rar                                                    \b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file sword_exercise.rar\n",
            "38602974 bytes, modified on 2011-08-25 12:24\n",
            "with a new one\n",
            "38602974 bytes, modified on 2011-08-25 12:24\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  sword_exercise.rar                                           \b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file talk.rar\n",
            "52343293 bytes, modified on 2011-08-25 12:25\n",
            "with a new one\n",
            "52343293 bytes, modified on 2011-08-25 12:25\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  talk.rar                                                     \b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file throw.rar\n",
            "33479582 bytes, modified on 2011-08-25 12:25\n",
            "with a new one\n",
            "33479582 bytes, modified on 2011-08-25 12:25\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  throw.rar                                                    \b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file turn.rar\n",
            "54143194 bytes, modified on 2011-08-25 12:26\n",
            "with a new one\n",
            "54143194 bytes, modified on 2011-08-25 12:26\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  turn.rar                                                     \b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file walk.rar\n",
            "177522918 bytes, modified on 2011-08-25 12:27\n",
            "with a new one\n",
            "177522918 bytes, modified on 2011-08-25 12:27\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  walk.rar                                                     \b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file wave.rar\n",
            "29439358 bytes, modified on 2011-08-25 12:28\n",
            "with a new one\n",
            "29439358 bytes, modified on 2011-08-25 12:28\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  wave.rar                                                     \b\b\b\b 32%\b\b\b\b 33%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file brush_hair.rar\n",
            "83671610 bytes, modified on 2011-08-25 12:28\n",
            "with a new one\n",
            "83671610 bytes, modified on 2011-08-25 12:28\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  brush_hair.rar                                               \b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file cartwheel.rar\n",
            "30689539 bytes, modified on 2011-08-25 12:29\n",
            "with a new one\n",
            "30689539 bytes, modified on 2011-08-25 12:29\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  cartwheel.rar                                                \b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file catch.rar\n",
            "17989454 bytes, modified on 2011-08-25 12:29\n",
            "with a new one\n",
            "17989454 bytes, modified on 2011-08-25 12:29\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  catch.rar                                                    \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file chew.rar\n",
            "33328463 bytes, modified on 2011-08-25 12:29\n",
            "with a new one\n",
            "33328463 bytes, modified on 2011-08-25 12:29\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  chew.rar                                                     \b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file clap.rar\n",
            "33579036 bytes, modified on 2011-08-25 12:30\n",
            "with a new one\n",
            "33579036 bytes, modified on 2011-08-25 12:30\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  clap.rar                                                     \b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file climb.rar\n",
            "55571348 bytes, modified on 2011-08-25 12:30\n",
            "with a new one\n",
            "55571348 bytes, modified on 2011-08-25 12:30\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  climb.rar                                                    \b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file climb_stairs.rar\n",
            "34598893 bytes, modified on 2011-08-25 12:30\n",
            "with a new one\n",
            "34598893 bytes, modified on 2011-08-25 12:30\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  climb_stairs.rar                                             \b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file dive.rar\n",
            "35254666 bytes, modified on 2011-08-25 12:31\n",
            "with a new one\n",
            "35254666 bytes, modified on 2011-08-25 12:31\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  dive.rar                                                     \b\b\b\b 47%\b\b\b\b 48%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file draw_sword.rar\n",
            "24393246 bytes, modified on 2011-08-25 12:31\n",
            "with a new one\n",
            "24393246 bytes, modified on 2011-08-25 12:31\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  draw_sword.rar                                               \b\b\b\b 49%\b\b\b\b 50%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file dribble.rar\n",
            "49973413 bytes, modified on 2011-08-25 12:31\n",
            "with a new one\n",
            "49973413 bytes, modified on 2011-08-25 12:31\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  dribble.rar                                                  \b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file drink.rar\n",
            "43571726 bytes, modified on 2011-08-25 12:32\n",
            "with a new one\n",
            "43571726 bytes, modified on 2011-08-25 12:32\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  drink.rar                                                    \b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file eat.rar\n",
            "28848032 bytes, modified on 2011-08-25 12:32\n",
            "with a new one\n",
            "28848032 bytes, modified on 2011-08-25 12:32\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  eat.rar                                                      \b\b\b\b 54%\b\b\b\b 55%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file fall_floor.rar\n",
            "29833121 bytes, modified on 2011-08-25 12:32\n",
            "with a new one\n",
            "29833121 bytes, modified on 2011-08-25 12:32\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  fall_floor.rar                                               \b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file fencing.rar\n",
            "31644199 bytes, modified on 2011-08-25 12:33\n",
            "with a new one\n",
            "31644199 bytes, modified on 2011-08-25 12:33\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  fencing.rar                                                  \b\b\b\b 57%\b\b\b\b 58%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file flic_flac.rar\n",
            "28613514 bytes, modified on 2011-08-25 12:33\n",
            "with a new one\n",
            "28613514 bytes, modified on 2011-08-25 12:33\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  flic_flac.rar                                                \b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file golf.rar\n",
            "33247924 bytes, modified on 2011-08-25 12:33\n",
            "with a new one\n",
            "33247924 bytes, modified on 2011-08-25 12:33\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  golf.rar                                                     \b\b\b\b 60%\b\b\b\b 61%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file handstand.rar\n",
            "31391690 bytes, modified on 2011-08-25 12:34\n",
            "with a new one\n",
            "31391690 bytes, modified on 2011-08-25 12:34\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  handstand.rar                                                \b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file hit.rar\n",
            "19335335 bytes, modified on 2011-08-25 12:34\n",
            "with a new one\n",
            "19335335 bytes, modified on 2011-08-25 12:34\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  hit.rar                                                      \b\b\b\b 63%\b\b\b\b 64%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file hug.rar\n",
            "37674171 bytes, modified on 2011-08-25 12:34\n",
            "with a new one\n",
            "37674171 bytes, modified on 2011-08-25 12:34\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  hug.rar                                                      \b\b\b\b 64%\b\b\b\b 65%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file jump.rar\n",
            "31152711 bytes, modified on 2011-08-25 12:35\n",
            "with a new one\n",
            "31152711 bytes, modified on 2011-08-25 12:35\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
            "\n",
            "\n",
            "Would you like to replace the existing file kick.rar\n",
            "26544777 bytes, modified on 2011-08-25 12:17\n",
            "with a new one\n",
            "26544777 bytes, modified on 2011-08-25 12:17\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  kick.rar                                                     \b\b\b\b 67%\b\b\b\b 68%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file kick_ball.rar\n",
            "27853717 bytes, modified on 2011-08-25 12:17\n",
            "with a new one\n",
            "27853717 bytes, modified on 2011-08-25 12:17\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  kick_ball.rar                                                \b\b\b\b 68%\b\b\b\b 69%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file kiss.rar\n",
            "38767725 bytes, modified on 2011-08-25 12:18\n",
            "with a new one\n",
            "38767725 bytes, modified on 2011-08-25 12:18\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  kiss.rar                                                     \b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file laugh.rar\n",
            "69736460 bytes, modified on 2011-08-25 12:18\n",
            "with a new one\n",
            "69736460 bytes, modified on 2011-08-25 12:18\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  laugh.rar                                                    \b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file pick.rar\n",
            "34449893 bytes, modified on 2011-08-25 12:18\n",
            "with a new one\n",
            "34449893 bytes, modified on 2011-08-25 12:18\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  pick.rar                                                     \b\b\b\b 75%\b\b\b\b 76%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file pour.rar\n",
            "47377717 bytes, modified on 2011-08-25 12:18\n",
            "with a new one\n",
            "47377717 bytes, modified on 2011-08-25 12:18\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  pour.rar                                                     \b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file pullup.rar\n",
            "30419364 bytes, modified on 2011-08-25 12:18\n",
            "with a new one\n",
            "30419364 bytes, modified on 2011-08-25 12:18\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  pullup.rar                                                   \b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file punch.rar\n",
            "31751850 bytes, modified on 2011-08-25 12:19\n",
            "with a new one\n",
            "31751850 bytes, modified on 2011-08-25 12:19\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  punch.rar                                                    \b\b\b\b 80%\b\b\b\b 81%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file push.rar\n",
            "35417913 bytes, modified on 2011-08-25 12:19\n",
            "with a new one\n",
            "35417913 bytes, modified on 2011-08-25 12:19\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  push.rar                                                     \b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file pushup.rar\n",
            "31743948 bytes, modified on 2011-08-25 12:19\n",
            "with a new one\n",
            "31743948 bytes, modified on 2011-08-25 12:19\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  pushup.rar                                                   \b\b\b\b 83%\b\b\b\b 84%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file ride_bike.rar\n",
            "40538444 bytes, modified on 2011-08-25 12:19\n",
            "with a new one\n",
            "40538444 bytes, modified on 2011-08-25 12:19\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  ride_bike.rar                                                \b\b\b\b 85%\b\b\b\b 86%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file ride_horse.rar\n",
            "60327697 bytes, modified on 2011-08-25 12:20\n",
            "with a new one\n",
            "60327697 bytes, modified on 2011-08-25 12:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  ride_horse.rar                                               \b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file run.rar\n",
            "62263592 bytes, modified on 2011-08-25 12:20\n",
            "with a new one\n",
            "62263592 bytes, modified on 2011-08-25 12:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  run.rar                                                      \b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shake_hands.rar\n",
            "50988394 bytes, modified on 2011-08-25 12:20\n",
            "with a new one\n",
            "50988394 bytes, modified on 2011-08-25 12:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shake_hands.rar                                              \b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_ball.rar\n",
            "38207595 bytes, modified on 2011-08-25 12:21\n",
            "with a new one\n",
            "38207595 bytes, modified on 2011-08-25 12:21\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_ball.rar                                               \b\b\b\b 95%\b\b\b\b 96%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_bow.rar\n",
            "67286443 bytes, modified on 2011-08-25 12:21\n",
            "with a new one\n",
            "67286443 bytes, modified on 2011-08-25 12:21\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_bow.rar                                                \b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "\n",
            "UNRAR 5.61 beta 1 freeware      Copyright (c) 1993-2018 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from shoot_gun.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file shoot_gun/2xFullautoGlocks_shoot_gun_u_cm_np1_le_med_0.avi\n",
            "303104 bytes, modified on 2010-12-08 14:36\n",
            "with a new one\n",
            "303104 bytes, modified on 2010-12-08 14:36\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_gun/2xFullautoGlocks_shoot_gun_u_cm_np1_le_med_0.avi     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_gun/American_History_X_shoot_gun_u_cm_np1_ba_med_36.avi\n",
            "232448 bytes, modified on 2010-12-08 10:16\n",
            "with a new one\n",
            "232448 bytes, modified on 2010-12-08 10:16\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_gun/American_History_X_shoot_gun_u_cm_np1_ba_med_36.avi     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_gun/American_History_X_shoot_gun_u_cm_np1_fr_med_2.avi\n",
            "103424 bytes, modified on 2010-12-08 10:16\n",
            "with a new one\n",
            "103424 bytes, modified on 2010-12-08 10:16\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_gun/American_History_X_shoot_gun_u_cm_np1_fr_med_2.avi     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_gun/American_History_X_shoot_gun_u_cm_np1_le_med_4.avi\n",
            "146944 bytes, modified on 2010-12-08 10:16\n",
            "with a new one\n",
            "146944 bytes, modified on 2010-12-08 10:16\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  shoot_gun/American_History_X_shoot_gun_u_cm_np1_le_med_4.avi     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file shoot_gun/American_History_X_shoot_gun_u_nm_np1_fr_goo_49.avi\n",
            "148992 bytes, modified on 2010-12-08 10:16\n",
            "with a new one\n",
            "148992 bytes, modified on 2010-12-08 10:16\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit q\n",
            "\n",
            "Program aborted\n",
            "\n",
            "UNRAR 5.61 beta 1 freeware      Copyright (c) 1993-2018 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from brush_hair.rar\n",
            "\n",
            "Creating    brush_hair                                                OK\n",
            "Extracting  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo_0.avi         \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo_1.avi         \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/April_09_brush_hair_u_nm_np1_ba_goo_2.avi         \b\b\b\b  1%\b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_0.avi     \b\b\b\b  2%\b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_1.avi     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ba_goo_4.avi     \b\b\b\b  3%\b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ri_med_3.avi     \b\b\b\b  4%\b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_0.avi     \b\b\b\b  5%\b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_1.avi     \b\b\b\b  6%\b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_med_2.avi     \b\b\b\b  7%\b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_ba_med_3.avi     \b\b\b\b  8%\b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b  9%\b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_goo_2.avi     \b\b\b\b 10%\b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_med_0.avi     \b\b\b\b 11%\b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Blonde_being_brushed_brush_hair_f_nm_np2_ri_med_0.avi     \b\b\b\b 12%\b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Blonde_being_brushed_brush_hair_u_cm_np2_ri_med_1.avi     \b\b\b\b 13%\b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_1.avi     \b\b\b\b 14%\b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_2.avi     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_4.avi     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_le_goo_3.avi     \b\b\b\b 16%\b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_ri_goo_0.avi     \b\b\b\b 17%\b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_h_cm_np2_ri_goo_6.avi     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_h_nm_np2_ri_goo_5.avi     \b\b\b\b 18%\b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_2_brush_hair_h_nm_np1_ba_med_0.avi     \b\b\b\b 19%\b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_2_brush_hair_h_nm_np1_ba_med_1.avi     \b\b\b\b 20%\b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_2_brush_hair_h_nm_np1_ba_med_2.avi     \b\b\b\b 21%\b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_brush_hair_f_cm_np2_ri_goo_0.avi     \b\b\b\b 22%\b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_brush_hair_f_nm_np2_ba_goo_2.avi     \b\b\b\b 23%\b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_brush_hair_f_nm_np2_ba_goo_4.avi     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_brush_hair_f_nm_np2_ri_goo_1.avi     \b\b\b\b 24%\b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_hair_brush_hair_f_nm_np2_ri_goo_3.avi     \b\b\b\b 25%\b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Hair_with_Beth_brush_hair_h_nm_np1_le_goo_0.avi     \b\b\b\b 26%\b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Hair_with_Beth_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Hair_with_Beth_brush_hair_u_nm_np1_fr_goo_2.avi     \b\b\b\b 27%\b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 28%\b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b 29%\b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_2.avi     \b\b\b\b 30%\b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_fr_goo_0.avi     \b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_1.avi     \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_2.avi     \b\b\b\b 34%\b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_3.avi     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_jrs_hair_brush_hair_u_cm_np2_le_goo_0.avi     \b\b\b\b 35%\b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_jrs_hair_brush_hair_u_cm_np2_le_goo_1.avi     \b\b\b\b 36%\b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_hair_-_December_2008_brush_hair_u_cm_np1_ba_goo_1.avi     \b\b\b\b 37%\b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_hair_-_December_2008_brush_hair_u_cm_np1_ba_goo_2.avi     \b\b\b\b 38%\b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_hair_-_December_2008_brush_hair_u_nm_np1_ba_goo_0.avi     \b\b\b\b 39%\b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_long_hair_brush_hair_u_nm_np1_ba_goo_2.avi     \b\b\b\b 40%\b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_long_hair_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 42%\b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_long_hair_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b 43%\b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_ba_goo_1.avi     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_ba_goo_2.avi     \b\b\b\b 44%\b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 45%\b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_0.avi     \b\b\b\b 46%\b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_1.avi     \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_2.avi     \b\b\b\b 47%\b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_0.avi     \b\b\b\b 49%\b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_1.avi     \b\b\b\b 50%\b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_2.avi     \b\b\b\b 51%\b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 52%\b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_2.avi     \b\b\b\b 53%\b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/can_somebody_just_brush_my_hair!_brush_hair_u_cm_np2_ri_med_0.avi     \b\b\b\b 54%\b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_fr_goo_2.avi     \b\b\b\b 55%\b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_le_goo_0.avi     \b\b\b\b 56%\b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_le_goo_1.avi     \b\b\b\b 57%\b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_nm_np1_ri_goo_3.avi     \b\b\b\b 58%\b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ba_goo_0.avi     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ba_goo_2.avi     \b\b\b\b 59%\b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ri_goo_1.avi     \b\b\b\b 60%\b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_ba_goo_0.avi     \b\b\b\b 62%\b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_ba_goo_2.avi     \b\b\b\b 63%\b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_le_goo_1.avi     \b\b\b\b 64%\b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Haarek_mmen_brush_hair_h_cm_np1_fr_goo_0.avi      \b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Haarek_mmen_brush_hair_h_cm_np1_fr_goo_1.avi      \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Haarek_mmen_brush_hair_h_cm_np1_fr_goo_2.avi      \b\b\b\b 67%\b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/indianrapunzels_com---silky_long_hair_brushing_brush_hair_u_cm_np2_fr_goo_0.avi     \b\b\b\b 68%\b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/indianrapunzels_com---silky_long_hair_brushing_brush_hair_u_cm_np2_ri_goo_1.avi     \b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_fr_med_1.avi     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_fr_med_2.avi     \b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_le_med_0.avi     \b\b\b\b 73%\b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/long_hair_Christa__brush_hair_u_nm_np1_le_goo_0.avi     \b\b\b\b 74%\b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_ba_goo_2.avi     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 75%\b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_le_goo_1.avi     \b\b\b\b 76%\b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_ri_goo_3.avi     \b\b\b\b 77%\b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/My_Hair_Routine_brush_hair_h_nm_np1_le_goo_0.avi     \b\b\b\b 78%\b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Olivia_Brushing_Hair_Playing_with_Evelyn_brush_hair_u_cm_np1_fr_goo_1.avi     \b\b\b\b 79%\b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Olivia_Brushing_Hair_Playing_with_Evelyn_brush_hair_u_cm_np2_ri_goo_0.avi     \b\b\b\b 80%\b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Prelinger_HabitPat1954_brush_hair_h_nm_np1_ba_med_2.avi     \b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Prelinger_HabitPat1954_brush_hair_h_nm_np1_fr_med_26.avi     \b\b\b\b 83%\b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Prelinger_HabitPat1954_brush_hair_u_cm_np1_le_med_10.avi     \b\b\b\b 84%\b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Prelinger_HabitPat1954_brush_hair_u_nm_np1_le_goo_11.avi     \b\b\b\b 85%\b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/rebecca_golden_hair_brush_hair_h_nm_np1_ba_goo_0.avi     \b\b\b\b 86%\b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/red_head_brush_hair_u_cm_np1_ba_goo_0.avi         \b\b\b\b 87%\b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/red_head_brush_hair_u_cm_np1_fr_goo_1.avi         \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/red_head_brush_hair_u_cm_np1_le_goo_2.avi         \b\b\b\b 88%\b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/sarah_brushing_her_hair_brush_hair_h_cm_np1_ri_goo_0.avi     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/sarah_brushing_her_hair_brush_hair_h_cm_np1_ri_goo_1.avi     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Silky_Straight_Hair_Original_brush_hair_h_nm_np1_ba_goo_0.avi     \b\b\b\b 90%\b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Silky_Straight_Hair_Original_brush_hair_h_nm_np1_ba_goo_1.avi     \b\b\b\b 91%\b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Silky_Straight_Hair_Original_brush_hair_u_nm_np1_ba_goo_2.avi     \b\b\b\b 92%\b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_0.avi     \b\b\b\b 93%\b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_1.avi     \b\b\b\b 94%\b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_2.avi     \b\b\b\b 95%\b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_0.avi     \b\b\b\b 96%\b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_1.avi     \b\b\b\b 97%\b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  brush_hair/Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_2.avi     \b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir_1 = './brush_hair'\n",
        "data_dir_2 = './shoot_gun'\n",
        "img_size = 64\n",
        "\n",
        "# Read frames from videos in the first directory\n",
        "data_1 = []\n",
        "for i, filename in enumerate(os.listdir(data_dir_1)):\n",
        "    if i == 5:\n",
        "        break\n",
        "    video_path = os.path.join(data_dir_1, filename)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (img_size, img_size))\n",
        "        data_1.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "# Read frames from videos in the second directory\n",
        "data_2 = []\n",
        "for i, filename in enumerate(os.listdir(data_dir_2)):\n",
        "    if i == 5:\n",
        "        break\n",
        "    video_path = os.path.join(data_dir_2, filename)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (img_size, img_size))\n",
        "        data_2.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "# Create labels for the data\n",
        "labels_1 = np.zeros(len(data_1))\n",
        "labels_2 = np.ones(len(data_2))\n",
        "\n",
        "# Combine the data and labels\n",
        "data = np.array(data_1 + data_2)\n",
        "labels = np.concatenate((labels_1, labels_2))\n",
        "\n",
        "# Shuffle the data and labels\n",
        "idx = np.random.permutation(len(data))\n",
        "data = data[idx]\n",
        "labels = labels[idx]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "split = int(0.8 * len(data))\n",
        "x_train = data[:split]\n",
        "y_train = labels[:split]\n",
        "x_test = data[split:]\n",
        "y_test = labels[split:]\n"
      ],
      "metadata": {
        "id": "NXJXoBdGqlhP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHFMWsLlqom2",
        "outputId": "b4397a39-372c-4290-948f-98169c4b1143"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "47/47 [==============================] - 13s 247ms/step - loss: 4.0656 - accuracy: 0.8879 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "47/47 [==============================] - 10s 208ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 9.8609e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "47/47 [==============================] - 11s 244ms/step - loss: 6.7742e-04 - accuracy: 1.0000 - val_loss: 4.0817e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "47/47 [==============================] - 11s 241ms/step - loss: 2.2440e-04 - accuracy: 1.0000 - val_loss: 2.8573e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "47/47 [==============================] - 11s 243ms/step - loss: 1.0139e-04 - accuracy: 1.0000 - val_loss: 7.2348e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "47/47 [==============================] - 11s 244ms/step - loss: 8.2795e-05 - accuracy: 1.0000 - val_loss: 7.8565e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "47/47 [==============================] - 10s 222ms/step - loss: 5.5211e-05 - accuracy: 1.0000 - val_loss: 5.5857e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "47/47 [==============================] - 11s 224ms/step - loss: 3.8122e-05 - accuracy: 1.0000 - val_loss: 4.0665e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "47/47 [==============================] - 11s 242ms/step - loss: 2.8965e-05 - accuracy: 1.0000 - val_loss: 3.0054e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "47/47 [==============================] - 11s 243ms/step - loss: 2.2254e-05 - accuracy: 1.0000 - val_loss: 2.7040e-05 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd522b6520>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the augmentations to be applied to the videos\n",
        "aug = vidaugs.Compose([\n",
        "    vidaugs.AddNoise(),\n",
        "    vidaugs.Blur(sigma=5.0),\n",
        "    vidaugs.OverlayDots(),\n",
        "])\n",
        "\n",
        "# Define the paths to the video files\n",
        "videos_path = \"./\"\n",
        "shoot_gun_path = os.path.join(videos_path, \"shoot_gun\")\n",
        "brush_hair_path = os.path.join(videos_path, \"brush_hair\")\n",
        "\n",
        "# Define the size of the video frames\n",
        "img_size = 224\n",
        "\n",
        "# Define the data lists\n",
        "data_1 = []\n",
        "labels_1 = []\n",
        "data_2 = []\n",
        "labels_2 = []\n"
      ],
      "metadata": {
        "id": "RP0EsB7Uqxog"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the first 5 videos from the shoot_gun folder\n",
        "for i, filename in enumerate(os.listdir(shoot_gun_path)[:5]):\n",
        "    if filename.endswith(\".avi\"):\n",
        "        filepath = os.path.join(shoot_gun_path, filename)\n",
        "        cap = cv2.VideoCapture(filepath)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, (img_size, img_size))\n",
        "            writer = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (img_size, img_size), True)\n",
        "            writer.write(frame)\n",
        "            writer.release()\n",
        "            aug_video = cv2.VideoCapture(\"output.avi\")\n",
        "            aug_frames = []\n",
        "            while aug_video.isOpened():\n",
        "                ret, frame = aug_video.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                aug_frames.append(frame)\n",
        "            aug_video.release()\n",
        "            os.remove(\"output.avi\")\n",
        "            aug_data = np.array(aug_frames)\n",
        "            data_1.append(aug_data)\n",
        "        cap.release()\n",
        "\n",
        "# Read the first 5 videos from the brush_hair folder\n",
        "for i, filename in enumerate(os.listdir(brush_hair_path)[:5]):\n",
        "    if filename.endswith(\".avi\"):\n",
        "        filepath = os.path.join(brush_hair_path, filename)\n",
        "        cap = cv2.VideoCapture(filepath)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, (img_size, img_size))\n",
        "            writer = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (img_size, img_size), True)\n",
        "            writer.write(frame)\n",
        "            writer.release()\n",
        "            aug_video = cv2.VideoCapture(\"output.avi\")\n",
        "            aug_frames = []\n",
        "            while aug_video.isOpened():\n",
        "                ret, frame = aug_video.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                aug_frames.append(frame)\n",
        "            aug_video.release()\n",
        "            os.remove(\"output.avi\")\n",
        "            aug_data = np.array(aug_frames)\n",
        "            data_2.append(aug_data)\n",
        "        cap.release()\n",
        "\n",
        "print(len(data_1))\n",
        "print(len(data_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GivQR0tq7tt",
        "outputId": "3189f203-4233-4039-8c60-02f4af86aadd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "382\n",
            "1492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data lists to numpy arrays\n",
        "data_1 = np.concatenate(data_1, axis=0)\n",
        "data_2 = np.concatenate(data_2, axis=0)\n",
        "\n",
        "\n",
        "# Create the labels for the data\n",
        "labels_1 = np.zeros((data_1.shape[0],))\n",
        "labels_2 = np.ones((data_2.shape[0],))\n",
        "\n",
        "# Merge the data and labels\n",
        "X = np.concatenate((data_1, data_2))\n",
        "y = np.concatenate((labels_1, labels_2))\n",
        "\n",
        "# Shuffle the data and labels\n",
        "p = np.random.permutation(X.shape[0])\n",
        "X = X[p]\n",
        "y = y[p]\n",
        "\n",
        "# Split the data and labels into training and testing sets\n",
        "split = int(0.8 * X.shape[0])\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]"
      ],
      "metadata": {
        "id": "rNEqJRbiq_SW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTvHT9HDq0GV",
        "outputId": "3a590b17-614b-4ec8-f853-cabc569d2bb0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1499, 224, 224, 3)\n",
            "(375, 224, 224, 3)\n",
            "(1499,)\n",
            "(375,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model architecture\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opDRMpD9q2om",
        "outputId": "bde4ef8d-fda5-4940-de80-a9ca8a74d86e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "47/47 [==============================] - 204s 4s/step - loss: 5.0199 - accuracy: 0.9320 - val_loss: 1.2889e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "47/47 [==============================] - 197s 4s/step - loss: 4.4040e-04 - accuracy: 1.0000 - val_loss: 1.8326e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "47/47 [==============================] - 198s 4s/step - loss: 0.0848 - accuracy: 0.9813 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "47/47 [==============================] - 188s 4s/step - loss: 0.0305 - accuracy: 0.9920 - val_loss: 0.0037 - val_accuracy: 0.9973\n",
            "Epoch 5/10\n",
            "47/47 [==============================] - 203s 4s/step - loss: 0.0060 - accuracy: 0.9967 - val_loss: 0.0019 - val_accuracy: 0.9973\n",
            "Epoch 6/10\n",
            "47/47 [==============================] - 197s 4s/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "47/47 [==============================] - 213s 5s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "47/47 [==============================] - 215s 5s/step - loss: 0.0053 - accuracy: 0.9993 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "47/47 [==============================] - 187s 4s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "47/47 [==============================] - 197s 4s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd4f4e25b0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gf-qF-Oq4g-",
        "outputId": "17d40737-5ce8-4813-d48e-3b326f8bcd0e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 13s 989ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timeseries"
      ],
      "metadata": {
        "id": "8qeSVJ6oDr5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
        "dataset = pd.read_csv(url, header=0, index_col=0)\n",
        "\n",
        "# Convert the dataset to a numpy array\n",
        "data = dataset.values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(len(data) * 0.67)\n",
        "test_size = len(data) - train_size\n",
        "train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
        "\n",
        "# Function to create the time series dataset\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Reshape the data into the appropriate format for the neural network\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "# Define the neural network model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(4, input_shape=(look_back, 1)),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Test loss:\", scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BkXEay9Dpyg",
        "outputId": "505044ce-892e-4cba-cd08-89afba4298f0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2441/2441 - 7s - loss: 57.4063 - 7s/epoch - 3ms/step\n",
            "Epoch 2/20\n",
            "2441/2441 - 5s - loss: 15.7742 - 5s/epoch - 2ms/step\n",
            "Epoch 3/20\n",
            "2441/2441 - 5s - loss: 9.4864 - 5s/epoch - 2ms/step\n",
            "Epoch 4/20\n",
            "2441/2441 - 6s - loss: 7.8869 - 6s/epoch - 2ms/step\n",
            "Epoch 5/20\n",
            "2441/2441 - 5s - loss: 7.3149 - 5s/epoch - 2ms/step\n",
            "Epoch 6/20\n",
            "2441/2441 - 6s - loss: 7.0568 - 6s/epoch - 2ms/step\n",
            "Epoch 7/20\n",
            "2441/2441 - 5s - loss: 6.8638 - 5s/epoch - 2ms/step\n",
            "Epoch 8/20\n",
            "2441/2441 - 5s - loss: 6.8072 - 5s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "2441/2441 - 6s - loss: 6.7023 - 6s/epoch - 2ms/step\n",
            "Epoch 10/20\n",
            "2441/2441 - 5s - loss: 6.6606 - 5s/epoch - 2ms/step\n",
            "Epoch 11/20\n",
            "2441/2441 - 5s - loss: 6.6241 - 5s/epoch - 2ms/step\n",
            "Epoch 12/20\n",
            "2441/2441 - 5s - loss: 6.5955 - 5s/epoch - 2ms/step\n",
            "Epoch 13/20\n",
            "2441/2441 - 5s - loss: 6.5841 - 5s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "2441/2441 - 6s - loss: 6.5591 - 6s/epoch - 2ms/step\n",
            "Epoch 15/20\n",
            "2441/2441 - 5s - loss: 6.5565 - 5s/epoch - 2ms/step\n",
            "Epoch 16/20\n",
            "2441/2441 - 5s - loss: 6.5357 - 5s/epoch - 2ms/step\n",
            "Epoch 17/20\n",
            "2441/2441 - 5s - loss: 6.5187 - 5s/epoch - 2ms/step\n",
            "Epoch 18/20\n",
            "2441/2441 - 5s - loss: 6.5292 - 5s/epoch - 2ms/step\n",
            "Epoch 19/20\n",
            "2441/2441 - 6s - loss: 6.5024 - 6s/epoch - 2ms/step\n",
            "Epoch 20/20\n",
            "2441/2441 - 5s - loss: 6.4938 - 5s/epoch - 2ms/step\n",
            "Test loss: 5.725090980529785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the augmentations\n",
        "augmenter = (\n",
        "    TimeWarp() \n",
        "    + Drift(max_drift=(0.1, 0.3)) \n",
        "    + Reverse()\n",
        ")\n",
        "\n",
        "# Apply the augmentations to the training data\n",
        "trainX_aug = np.zeros_like(trainX)\n",
        "for i in range(len(trainX)):\n",
        "    trainX_aug[i, :, 0] = augmenter.augment(trainX[i, :, 0])\n",
        "    \n",
        "# Apply the augmentations to the testing data\n",
        "testX_aug = np.zeros_like(testX)\n",
        "for i in range(len(testX)):\n",
        "    testX_aug[i, :, 0] = augmenter.augment(testX[i, :, 0])"
      ],
      "metadata": {
        "id": "st5NZ7StDwU5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the augmented training data\n",
        "model.fit(trainX_aug, trainY, epochs=20, batch_size=1, verbose=2)\n",
        "\n",
        "# Evaluate the model on the augmented testing data\n",
        "scores_aug = model.evaluate(testX_aug, testY, verbose=0)\n",
        "print(\"Test loss (augmented data):\", scores_aug)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKdnopAjDyXy",
        "outputId": "c218fcb3-3875-429b-fa48-f08f6530b3d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2441/2441 - 9s - loss: 8.8029 - 9s/epoch - 4ms/step\n",
            "Epoch 2/20\n",
            "2441/2441 - 5s - loss: 7.8154 - 5s/epoch - 2ms/step\n",
            "Epoch 3/20\n",
            "2441/2441 - 6s - loss: 7.4775 - 6s/epoch - 2ms/step\n",
            "Epoch 4/20\n",
            "2441/2441 - 5s - loss: 7.2722 - 5s/epoch - 2ms/step\n",
            "Epoch 5/20\n",
            "2441/2441 - 6s - loss: 7.1028 - 6s/epoch - 2ms/step\n",
            "Epoch 6/20\n",
            "2441/2441 - 5s - loss: 7.0296 - 5s/epoch - 2ms/step\n",
            "Epoch 7/20\n",
            "2441/2441 - 5s - loss: 6.9428 - 5s/epoch - 2ms/step\n",
            "Epoch 8/20\n",
            "2441/2441 - 6s - loss: 6.9600 - 6s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "2441/2441 - 5s - loss: 6.9180 - 5s/epoch - 2ms/step\n",
            "Epoch 10/20\n",
            "2441/2441 - 5s - loss: 6.8955 - 5s/epoch - 2ms/step\n",
            "Epoch 11/20\n",
            "2441/2441 - 5s - loss: 6.8526 - 5s/epoch - 2ms/step\n",
            "Epoch 12/20\n",
            "2441/2441 - 5s - loss: 6.8624 - 5s/epoch - 2ms/step\n",
            "Epoch 13/20\n",
            "2441/2441 - 6s - loss: 6.8626 - 6s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "2441/2441 - 5s - loss: 6.8595 - 5s/epoch - 2ms/step\n",
            "Epoch 15/20\n",
            "2441/2441 - 5s - loss: 6.8405 - 5s/epoch - 2ms/step\n",
            "Epoch 16/20\n",
            "2441/2441 - 5s - loss: 6.8195 - 5s/epoch - 2ms/step\n",
            "Epoch 17/20\n",
            "2441/2441 - 5s - loss: 6.8434 - 5s/epoch - 2ms/step\n",
            "Epoch 18/20\n",
            "2441/2441 - 6s - loss: 6.8254 - 6s/epoch - 2ms/step\n",
            "Epoch 19/20\n",
            "2441/2441 - 5s - loss: 6.8099 - 5s/epoch - 2ms/step\n",
            "Epoch 20/20\n",
            "2441/2441 - 5s - loss: 6.8358 - 5s/epoch - 2ms/step\n",
            "Test loss (augmented data): 6.306217193603516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJ4gUHdaHKYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}